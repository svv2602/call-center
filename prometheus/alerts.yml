groups:
  - name: call_center_alerts
    rules:
      # High transfer rate: >50% in last hour
      - alert: HighTransferRate
        expr: >
          (
            sum(increase(callcenter_transfers_to_operator_total[1h]))
            /
            sum(increase(callcenter_calls_total[1h]))
          ) > 0.5
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "High operator transfer rate"
          description: >
            Transfer rate is {{ $value | humanizePercentage }} over the last hour.
            Threshold: 50%.

      # STT/TTS/LLM errors: >5 in 10 minutes
      - alert: PipelineErrorsHigh
        expr: >
          sum(increase(callcenter_calls_total{status="error"}[10m])) > 5
        for: 1m
        labels:
          severity: high
        annotations:
          summary: "High pipeline error rate"
          description: >
            {{ $value }} errors in the last 10 minutes.
            Threshold: 5 errors per 10 minutes.

      # Response latency p95 > 2.5 seconds (per NFR spec)
      - alert: HighResponseLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(callcenter_total_response_latency_ms_bucket[5m])) by (le)
          ) > 2500
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: "High response latency (p95)"
          description: >
            p95 response latency is {{ $value | humanize }}ms.
            Threshold: 2500ms.

      # STT latency p95 > 700ms
      - alert: HighSTTLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(callcenter_stt_latency_ms_bucket[5m])) by (le)
          ) > 700
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: "High STT latency (p95)"
          description: "p95 STT latency is {{ $value | humanize }}ms. Threshold: 700ms."

      # LLM latency p95 > 1500ms
      - alert: HighLLMLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(callcenter_llm_latency_ms_bucket[5m])) by (le)
          ) > 1500
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: "High LLM latency (p95)"
          description: "p95 LLM latency is {{ $value | humanize }}ms. Threshold: 1500ms."

      # TTS latency p95 > 600ms
      - alert: HighTTSLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(callcenter_tts_latency_ms_bucket[5m])) by (le)
          ) > 600
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: "High TTS latency (p95)"
          description: "p95 TTS latency is {{ $value | humanize }}ms. Threshold: 600ms."

      # AudioSocket → STT latency p95 > 100ms
      - alert: HighAudioSocketToSTTLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(callcenter_audiosocket_to_stt_ms_bucket[5m])) by (le)
          ) > 100
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: "High AudioSocket-to-STT latency (p95)"
          description: "p95 AudioSocket→STT latency is {{ $value | humanize }}ms. Threshold: 100ms."

      # TTS → AudioSocket delivery latency p95 > 100ms
      - alert: HighTTSDeliveryLatency
        expr: >
          histogram_quantile(0.95,
            sum(rate(callcenter_tts_delivery_ms_bucket[5m])) by (le)
          ) > 100
        for: 5m
        labels:
          severity: medium
        annotations:
          summary: "High TTS delivery latency (p95)"
          description: "p95 TTS→AudioSocket delivery latency is {{ $value | humanize }}ms. Threshold: 100ms."

      # Operator queue > 5 — no available operators
      - alert: OperatorQueueOverflow
        expr: callcenter_operator_queue_length > 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Operator queue overflow"
          description: >
            {{ $value }} calls waiting in operator queue.
            Threshold: 5. Check operator availability.

      # Abnormal API spend: >200% of daily average
      - alert: AbnormalAPISpend
        expr: >
          sum(increase(callcenter_call_cost_usd_sum[1h]))
          >
          2 * avg_over_time(sum(increase(callcenter_call_cost_usd_sum[1h]))[24h:1h])
        for: 15m
        labels:
          severity: high
        annotations:
          summary: "Abnormal API spend detected"
          description: >
            Hourly API spend is more than 200% of the 24h average.
            Current: ${{ $value | humanize }}/hour.

      # Prompt injection suspicion: anomalous tool calls
      - alert: SuspiciousToolCalls
        expr: >
          sum(rate(callcenter_store_api_errors_total{status_code="400"}[10m])) > 0.5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Suspicious tool call pattern detected"
          description: >
            High rate of 400 errors from Store API ({{ $value | humanize }}/s).
            May indicate prompt injection or anomalous tool call parameters.

      # Celery worker down: no workers responding to ping
      - alert: CeleryWorkerDown
        expr: callcenter_celery_workers_online == 0
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "Celery worker is down"
          description: >
            No Celery workers responding to ping for >5 minutes.
            Check: docker compose logs celery-worker

      # Store API circuit breaker open
      - alert: CircuitBreakerOpen
        expr: callcenter_store_api_circuit_breaker_state == 1
        for: 1m
        labels:
          severity: high
        annotations:
          summary: "Store API circuit breaker is OPEN"
          description: >
            Circuit breaker has tripped. Store API calls are being blocked.
            Will auto-recover after timeout period.

  - name: backup_partition_alerts
    rules:
      # PostgreSQL backup stale: no successful backup in 26 hours
      - alert: PostgresBackupStale
        expr: >
          (time() - callcenter_backup_last_success_timestamp{component="postgres"}) > 93600
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL backup is stale"
          description: >
            Last successful PostgreSQL backup was {{ $value | humanizeDuration }} ago.
            Expected: daily at 04:00. Check Celery Beat and worker logs.

      # Redis backup stale: no successful backup in 26 hours
      - alert: RedisBackupStale
        expr: >
          (time() - callcenter_backup_last_success_timestamp{component="redis"}) > 93600
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "Redis backup is stale"
          description: >
            Last successful Redis backup was {{ $value | humanizeDuration }} ago.
            Expected: daily at 04:15.

      # Knowledge base backup stale: no successful backup in 8 days
      - alert: KnowledgeBackupStale
        expr: >
          (time() - callcenter_backup_last_success_timestamp{component="knowledge"}) > 691200
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "Knowledge base backup is stale"
          description: >
            Last successful knowledge base backup was {{ $value | humanizeDuration }} ago.
            Expected: weekly on Sunday at 01:00.

      # Backup errors: any backup failure
      - alert: BackupFailed
        expr: increase(callcenter_backup_errors_total[1h]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Backup failed for {{ $labels.component }}"
          description: >
            Backup task for {{ $labels.component }} failed.
            Check Celery worker logs: docker compose logs celery-worker.

      # Partition management stale: no run in 35 days
      - alert: PartitionCreationStale
        expr: >
          (time() - callcenter_partition_last_success_timestamp) > 3024000
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "Partition management has not run recently"
          description: >
            Last successful partition management was {{ $value | humanizeDuration }} ago.
            Expected: monthly on the 1st at 02:00.

      # Partition errors
      - alert: PartitionManagementFailed
        expr: increase(callcenter_partition_errors_total[1h]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Partition management failed"
          description: >
            Partition creation/cleanup task failed.
            Check: docker compose logs celery-worker.
